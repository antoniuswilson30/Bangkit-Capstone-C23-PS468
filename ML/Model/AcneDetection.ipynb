{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained Model EfficientNetB1 without Top Layer\n",
    "# !wget \"https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB1\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained Model EfficientNetB1 without Top Layer\n",
    "!wget \"https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB1\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pretrained model as transfer learning layers\n",
    "local_weight_file = './pretrained/efficientnetb1_notop.h5'\n",
    "\n",
    "#Adjust input shape and weights\n",
    "pre_trained_model = EfficientNetB1(input_shape = (224, 224, 3), \n",
    "                                include_top = False, \n",
    "                                weights = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the downloaded pre-trained weights\n",
    "pre_trained_model.load_weights(local_weight_file)\n",
    "\n",
    "# Freeze the weights of the layers.\n",
    "for layer in pre_trained_model.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape:  [(None, 224, 224, 3)]\n"
     ]
    }
   ],
   "source": [
    "#Specify input layer model\n",
    "last_layer = pre_trained_model.get_layer('input_1')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 150528)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               77070848  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,071,361\n",
      "Trainable params: 77,071,361\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 150528)            0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1024)              154141696 \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 154,142,721\n",
      "Trainable params: 154,142,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(last_output)\n",
    "# Add a half connected layer with 512 hidden units and ReLU activation\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "# Add a dropout rate of 0.3\n",
    "x = layers.Dropout(0.3)(x)                  \n",
    "# Add a final sigmoid layer for classification, sigmoid are used because the model only have 2 ouputs\n",
    "x = layers.Dense(1, activation='sigmoid')(x)           \n",
    "x = layers.Dense (1, activation='sigmoid')(x)           \n",
    "\n",
    "# Append the dense network to the base model\n",
    "acneModel = Model(pre_trained_model.input, x) \n",
    "\n",
    "# Print the model summary\n",
    "acneModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiled the model using adam optimizer and binary classification\n",
    "acneModel.compile(optimizer = Adam(learning_rate=0.0001), \n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 408 images belonging to 2 classes.\n",
      "Found 159 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../Dataset/'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with training acne pictures\n",
    "train_acne_dir = os.path.join(train_dir, 'acne3') \n",
    "\n",
    "# Directory with training normal pictures\n",
    "train_normal_dir = os.path.join(train_dir, 'normal2') \n",
    "\n",
    "# Directory with validation acne pictures\n",
    "validation_acne_dir = os.path.join(validation_dir, 'acne2') \n",
    "\n",
    "# Directory with validation normal pictures\n",
    "validation_normal_dir = os.path.join(validation_dir, 'normal2')\n",
    "\n",
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    classes=['acne3', 'normal2'],\n",
    "                                                    batch_size = 20,\n",
    "                                                    class_mode = 'binary', \n",
    "                                                    target_size = (224, 224))     \n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
    "                                                         classes=['acne2', 'normal2'],\n",
    "                                                          batch_size  = 20,\n",
    "                                                          class_mode  = 'binary', \n",
    "                                                          target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining callback for preventing underfitting and overfitting\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('accuracy') is not None and (logs.get('accuracy') > 0.99 or logs.get('accuracy') < 0.5):\n",
    "            print(\"\\nCancelling training\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 14s 633ms/step - loss: 2.8030 - accuracy: 0.6642 - val_loss: 1.6881 - val_accuracy: 0.6855\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 16s 745ms/step - loss: 2.8073 - accuracy: 0.6569 - val_loss: 1.9883 - val_accuracy: 0.4340\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 12s 533ms/step - loss: 2.4677 - accuracy: 0.6691 - val_loss: 0.8878 - val_accuracy: 0.6289\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 9s 440ms/step - loss: 1.0574 - accuracy: 0.7083 - val_loss: 0.6106 - val_accuracy: 0.6918\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 9s 444ms/step - loss: 0.7000 - accuracy: 0.7034 - val_loss: 0.6129 - val_accuracy: 0.6541\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 9s 435ms/step - loss: 0.5154 - accuracy: 0.7353 - val_loss: 0.6816 - val_accuracy: 0.6855\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 9s 413ms/step - loss: 0.5386 - accuracy: 0.7647 - val_loss: 0.5667 - val_accuracy: 0.6415\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 14s 696ms/step - loss: 0.4838 - accuracy: 0.7819 - val_loss: 0.5659 - val_accuracy: 0.6855\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 18s 867ms/step - loss: 0.5123 - accuracy: 0.7647 - val_loss: 0.6690 - val_accuracy: 0.6855\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 18s 848ms/step - loss: 0.5069 - accuracy: 0.7770 - val_loss: 0.5715 - val_accuracy: 0.6478\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 16s 721ms/step - loss: 0.5194 - accuracy: 0.7574 - val_loss: 0.5867 - val_accuracy: 0.6855\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 14s 663ms/step - loss: 0.5279 - accuracy: 0.7794 - val_loss: 0.5799 - val_accuracy: 0.6541\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 10s 490ms/step - loss: 0.5207 - accuracy: 0.7794 - val_loss: 0.5768 - val_accuracy: 0.6792\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 10s 471ms/step - loss: 0.4847 - accuracy: 0.7819 - val_loss: 0.5547 - val_accuracy: 0.6667\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 10s 454ms/step - loss: 0.4803 - accuracy: 0.7696 - val_loss: 0.5476 - val_accuracy: 0.6855\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 9s 442ms/step - loss: 0.4446 - accuracy: 0.7745 - val_loss: 0.5295 - val_accuracy: 0.6415\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 10s 446ms/step - loss: 0.4564 - accuracy: 0.7843 - val_loss: 0.6514 - val_accuracy: 0.6855\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 9s 443ms/step - loss: 0.4488 - accuracy: 0.7794 - val_loss: 0.5225 - val_accuracy: 0.6981\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 10s 461ms/step - loss: 0.5078 - accuracy: 0.7843 - val_loss: 0.6553 - val_accuracy: 0.6855\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 10s 454ms/step - loss: 0.4552 - accuracy: 0.7868 - val_loss: 0.5745 - val_accuracy: 0.6918\n",
      "22/22 [==============================] - 19s 816ms/step - loss: 4.8579 - accuracy: 0.6745 - val_loss: 1.5271 - val_accuracy: 0.3082\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 16s 734ms/step - loss: 1.3732 - accuracy: 0.6439 - val_loss: 0.6232 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 16s 715ms/step - loss: 0.6937 - accuracy: 0.6958 - val_loss: 0.6147 - val_accuracy: 0.6289\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 16s 702ms/step - loss: 0.6034 - accuracy: 0.7288 - val_loss: 0.5893 - val_accuracy: 0.6730\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 16s 713ms/step - loss: 0.5256 - accuracy: 0.7618 - val_loss: 0.6191 - val_accuracy: 0.6855\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 16s 707ms/step - loss: 0.5429 - accuracy: 0.7642 - val_loss: 0.5944 - val_accuracy: 0.6918\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 16s 711ms/step - loss: 0.5358 - accuracy: 0.7689 - val_loss: 0.5723 - val_accuracy: 0.6415\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 16s 719ms/step - loss: 0.5069 - accuracy: 0.7665 - val_loss: 0.6071 - val_accuracy: 0.6667\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 16s 706ms/step - loss: 0.5059 - accuracy: 0.7807 - val_loss: 0.5724 - val_accuracy: 0.6855\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 15s 693ms/step - loss: 0.4627 - accuracy: 0.7712 - val_loss: 0.5580 - val_accuracy: 0.6415\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 16s 709ms/step - loss: 0.4716 - accuracy: 0.7571 - val_loss: 0.5508 - val_accuracy: 0.6918\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 16s 700ms/step - loss: 0.5049 - accuracy: 0.7571 - val_loss: 0.6586 - val_accuracy: 0.6855\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 16s 705ms/step - loss: 0.4869 - accuracy: 0.7665 - val_loss: 0.5754 - val_accuracy: 0.6541\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 16s 705ms/step - loss: 0.5065 - accuracy: 0.7807 - val_loss: 0.5469 - val_accuracy: 0.6730\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 16s 702ms/step - loss: 0.4943 - accuracy: 0.7311 - val_loss: 0.5515 - val_accuracy: 0.6667\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 16s 711ms/step - loss: 0.4636 - accuracy: 0.7524 - val_loss: 0.5676 - val_accuracy: 0.6541\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 16s 699ms/step - loss: 0.4841 - accuracy: 0.7217 - val_loss: 0.5398 - val_accuracy: 0.6981\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 16s 713ms/step - loss: 0.4629 - accuracy: 0.7642 - val_loss: 0.6707 - val_accuracy: 0.6918\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 21s 949ms/step - loss: 0.4823 - accuracy: 0.7642 - val_loss: 0.5397 - val_accuracy: 0.7044\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 16s 703ms/step - loss: 0.4631 - accuracy: 0.7571 - val_loss: 0.5414 - val_accuracy: 0.6855\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "callbacks = myCallback()\n",
    "history = acneModel.fit(\n",
    "            train_generator,\n",
    "            validation_data = validation_generator,\n",
    "            epochs = 20,\n",
    "            callbacks=[callbacks])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "no acne [45.994083] %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "# from google.colab import files\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "uploaded = filedialog.askopenfilename()\n",
    " \n",
    "  # predicting images\n",
    "path = uploaded\n",
    "img = load_img(path, target_size=(224, 224))\n",
    "x = img_to_array(img)\n",
    "x /= 255\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "images = np.vstack([x])\n",
    "classes = acneModel.predict(images, batch_size=10)\n",
    "\n",
    "if classes[0]>0.5:\n",
    "  print(\"no dullness/acne\")\n",
    "  print()\n",
    "else:\n",
    "  print(\"dullness/acne\")\n",
    "  print()\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dump = {\n",
    "    'model':acneModel,\n",
    "}\n",
    "# Save your model to a file using pickle\n",
    "with open(\"scanningmodel.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dump, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPS_SAVED_MODEL = \"rps_saved_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, RPS_SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(RPS_SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(loaded.signatures.keys()))\n",
    "infer = loaded.signatures[\"serving_default\"]\n",
    "print(infer.structured_input_signature)\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(RPS_SAVED_MODEL)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_file = 'converted_model.tflite'\n",
    "\n",
    "with open(tflite_model_file, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "with open(tflite_model_file, 'rb') as fid:\n",
    "    tflite_model = fid.read()\n",
    "    \n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results for the randomly sampled test images\n",
    "predictions = []\n",
    "\n",
    "test_labels, test_imgs = [], []\n",
    "for img, label in tqdm(test_batches.take(10)):\n",
    "    interpreter.set_tensor(input_index, img)\n",
    "    interpreter.invoke()\n",
    "    predictions.append(interpreter.get_tensor(output_index))\n",
    "    \n",
    "    test_labels.append(label.numpy()[0])\n",
    "    test_imgs.append(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
